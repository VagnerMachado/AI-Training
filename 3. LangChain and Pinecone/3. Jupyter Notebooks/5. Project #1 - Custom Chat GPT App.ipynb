{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "029c8a52-b02e-46fb-a2f6-04da54a15ae0",
   "metadata": {},
   "source": [
    "# Advantages of using a custon chat GPT\n",
    "\n",
    "- Increased security but enabling authentication to access company chat\n",
    "- Improved user experience by creating use cases specific to a company need\n",
    "- Greater flexifiblity by adding custom fallbacks to user inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74a606c6-c87f-40b3-9a63-18ed06a8de9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2af2ba68-d98f-48b9-b4db-72196ff06c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e3bfcc5-8dab-4593-af23-d71e57ff6a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vagner Machado\\AppData\\Local\\Temp\\ipykernel_28828\\2034590030.py:10: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  chain = LLMChain(\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your prompt What makes the sky blue during the day and red around dusk and dawn?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'What makes the sky blue during the day and red around dusk and dawn?', 'text': \"During the day, the sky appears blue because of the way sunlight is scattered in the Earth's atmosphere. The gases and particles in the atmosphere scatter sunlight in all directions, but shorter (blue) wavelengths are scattered more than longer (red) wavelengths due to Rayleigh scattering. This causes the blue light to be scattered all around the sky, making it appear blue to our eyes.\\n\\nDuring sunrise and sunset, the sunlight has to pass through a larger portion of the Earth's atmosphere compared to when the sun is directly overhead. This means that more of the shorter blue and green wavelengths are scattered out, and the longer red and orange wavelengths are able to reach our eyes more directly. This is why the sky appears red or orange during these times, as the shorter wavelengths are scattered away and the longer wavelengths dominate the colors we see.\"}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your prompt So what can be done to change the colors?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'So what can be done to change the colors?', 'text': 'To change the colors of what specifically? Can you provide more context or details so I can assist you better?'}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your prompt bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goodbye!\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=1)\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"content\"],\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a chatbot having a conversation with a human\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=False\n",
    ")\n",
    "while True:\n",
    "    content = input(\"Please enter your prompt\")\n",
    "    if content in [\"quit\",\"exit\", \"bye\"]:\n",
    "        print(\"goodbye!\")\n",
    "        break\n",
    "    response = chain.invoke({\"content\":content})\n",
    "    print(response)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241a8a99-8c20-44a9-baea-5c6fafbf4fab",
   "metadata": {},
   "source": [
    "## As you can see there is no memory from one question to another!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d40c903-7bed-48e8-8e0b-82969f07e854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before workin on that , lets test the System Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb99931d-13f1-4beb-ba72-10872494203c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your prompt What are the top 5 most delicious desserts in Spanish cousine?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'What are the top 5 most delicious desserts in Spanish cousine?', 'text': \"Les 5 desserts les plus délicieux de la cuisine espagnole sont la crème catalane, les churros, le flan, la tarta de Santiago et les pestiños. Avez-vous déjà goûté l'un de ces desserts espagnols?\"}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your prompt quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goodbye!\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=1)\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"content\"],\n",
    "    messages=[\n",
    "        # SystemMessage(content=\"You are a chatbot having a conversation with a human\"),\n",
    "        SystemMessage(content=\"You are a chatbot having a conversation with a human. Respond only in French.\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=False\n",
    ")\n",
    "while True:\n",
    "    content = input(\"Please enter your prompt\")\n",
    "    if content in [\"quit\",\"exit\", \"bye\"]:\n",
    "        print(\"Gboodbye!\")\n",
    "        break\n",
    "    response = chain.invoke({\"content\":content})\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08845df0-b407-4e0f-95c4-c5b8013fa2f6",
   "metadata": {},
   "source": [
    "# Adding Chat Memory Using ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b2f9e3f-8fba-4fba-babe-898a287f300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory # To enable buffer\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder # last param added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce94b576-0dad-4660-99c6-9857366f5c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your prompt what is the most common gas in earth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'what is the most common gas in earth', 'chat_history': [HumanMessage(content='what is the most common gas in earth', additional_kwargs={}, response_metadata={}), AIMessage(content='El gas más común en la Tierra es el nitrógeno, que constituye aproximadamente el 78% de la atmósfera terrestre. ¡Es fundamental para mantener el equilibrio de nuestro planeta!', additional_kwargs={}, response_metadata={})], 'text': 'El gas más común en la Tierra es el nitrógeno, que constituye aproximadamente el 78% de la atmósfera terrestre. ¡Es fundamental para mantener el equilibrio de nuestro planeta!'}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your prompt what is the diameter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'what is the diameter', 'chat_history': [HumanMessage(content='what is the most common gas in earth', additional_kwargs={}, response_metadata={}), AIMessage(content='El gas más común en la Tierra es el nitrógeno, que constituye aproximadamente el 78% de la atmósfera terrestre. ¡Es fundamental para mantener el equilibrio de nuestro planeta!', additional_kwargs={}, response_metadata={}), HumanMessage(content='what is the diameter', additional_kwargs={}, response_metadata={}), AIMessage(content='El diámetro de la Tierra es de aproximadamente 12,742 kilómetros. ¡Nuestro planeta es realmente grande e impresionante!', additional_kwargs={}, response_metadata={})], 'text': 'El diámetro de la Tierra es de aproximadamente 12,742 kilómetros. ¡Nuestro planeta es realmente grande e impresionante!'}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your prompt quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=1)\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"content\"],\n",
    "    messages=[\n",
    "        # SystemMessage(content=\"You are a chatbot having a conversation with a human\"),\n",
    "        SystemMessage(content=\"You are a chatbot having a conversation with a human. Respond only in Spanish.\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),   # Injectd this here\n",
    "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")\n",
    "while True:\n",
    "    content = input(\"Please enter your prompt\")\n",
    "    if content in [\"quit\",\"exit\", \"bye\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    response = chain.invoke({\"content\":content})\n",
    "    print(response)\n",
    "\n",
    "# As seen below the chat has history and can use that as the context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfae5e0-852e-463f-8be4-c4868a1f3596",
   "metadata": {},
   "source": [
    "# Saving Chat Sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba4a802-259b-4620-b836-ba63f824265c",
   "metadata": {},
   "source": [
    "To accomplish this feature, we will save our chat history to a file ( which can also be saved to a database) just before exiting the application. \n",
    "When the application is started again, those files (or database entries) are read and populated - giving the session history capability to the user. Lets keep on building the above app with memory, note new additions are commented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60784e4a-70e0-4e94-b701-323ba8ec5303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory, FileChatMessageHistory # Added last parameter so to enable file as data sorce for sessions.\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60cfc97-fdbb-48fb-8144-9744b8273b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=1)\n",
    "\n",
    "history = FileChatMessageHistory('chat_history_session.json') # create the file \n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\", \n",
    "    chat_memory = history,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"content\"],\n",
    "    messages=[\n",
    "        # SystemMessage(content=\"You are a chatbot having a conversation with a human\"),\n",
    "        SystemMessage(content=\"You are a chatbot having a conversation with a human. Respond only in Spanish.\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),   # Injectd this here\n",
    "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")\n",
    "while True:\n",
    "    content = input(\"Please enter your prompt\")\n",
    "    if content in [\"quit\",\"exit\", \"bye\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    response = chain.invoke({\"content\":content})\n",
    "    print(response)\n",
    "\n",
    "# As seen below the chat has history and can use that as the context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
