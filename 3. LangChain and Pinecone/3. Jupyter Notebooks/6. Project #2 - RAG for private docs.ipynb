{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca4268c0-6769-4bf1-b46b-90f63207c03a",
   "metadata": {},
   "source": [
    "# RAG for private documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2866d577-8433-4a77-9e1f-c5cdd85cb7eb",
   "metadata": {},
   "source": [
    "How can LLMs learn new knowledge? \n",
    "\n",
    "    - Fine tuning on a training set\n",
    "    - MOdel inputs\n",
    "\n",
    "The recommended approach is to use model with embedded-based search.\n",
    "\n",
    "1. Prepare the search data\n",
    "    - load data into langchain documents\n",
    "    - split the document into chunks\n",
    "    - embed the chunks into numeric vectors\n",
    "    - save the chunks and embeddings to a vector database\n",
    "      \n",
    "2. Search, once per query\n",
    "    - Embed a user's question\n",
    "    - Using the question embedding and the chunk embeddings, rank the vectors by similarity to the question embedding, whee the neares vector represents chunks most relevant.\n",
    "\n",
    "3. Ask\n",
    "    - Insert the question and the most relevant chunks into a message to a GPT model\n",
    "    - Return GPT answer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c7d148-8e29-4b3c-81db-d71481b6fa87",
   "metadata": {},
   "source": [
    "# Load the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d86e4d6-e033-48c7-ae0f-25c6e74d7167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q pypdf  #already installed so commenting out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112e1202-29b0-4a2b-b6cc-767e0edf1496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q docx2txt   #already installed so commenting out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc7f0ab9-229f-490c-8982-da08bd4eafa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install wikipedia -q   # alrady loaded so commenting out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97705749-3457-4e30-8f9a-2c3592050ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "\n",
    "###############  loading PDF, DOCX and TXT files as LangChain Documents\n",
    "def load_document(file):\n",
    "    import os\n",
    "    name, extension = os.path.splitext(file)\n",
    "\n",
    "    if extension == '.pdf':\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = PyPDFLoader(file) # cam also be used as pointer to online pdf in arg\n",
    "    elif extension == '.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = Docx2txtLoader(file)\n",
    "    # elif extension == '.txt':\n",
    "    #     from langchain.document_loaders import TextLoader\n",
    "    #     loader = TextLoader(file)\n",
    "    else:\n",
    "        print('Document format is not supported!')\n",
    "        return None\n",
    "\n",
    "    data = loader.load()\n",
    "    return data\n",
    "\n",
    "\n",
    "##############  Loading the data from wikipedia\n",
    "def load_from_wikipedia(query, lang=\"en\",load_max_docs=2):\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)\n",
    "    data = loader.load()\n",
    "    return data\n",
    "\n",
    "############# Data chunking before embedding int vector database\n",
    "def chunk_data(data, chunk_size=256):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter # recommended for langchain : tries to split \\\\n \\n and whitespace\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks\n",
    "\n",
    "############ Calculate the cost of embedding\n",
    "def print_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-3-small')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.00002:.6f}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd3c06f-7aea-4d46-a8f1-7574d553d1b5",
   "metadata": {},
   "source": [
    "# Embedding and Uploading a Vector Database to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c175b9bd-5676-4476-8b54-cd64c5327a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_or_fetch_embeddings(index_name, chunks):\n",
    "    # importing the necessary libraries and initializing the Pinecone client\n",
    "    import pinecone\n",
    "    from langchain_community.vectorstores import Pinecone\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    from pinecone import PodSpec\n",
    "\n",
    "    \n",
    "    pc = pinecone.Pinecone()\n",
    "        \n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)  # 512 works as well\n",
    "\n",
    "    # loading from existing index\n",
    "    if index_name in pc.list_indexes():\n",
    "        print(f'Index {index_name} already exists. Loading embeddings ... ', end='')\n",
    "        vector_store = Pinecone.from_existing_index(index_name, embeddings)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        # creating the index and embedding the chunks into the index \n",
    "        print(f'Creating index {index_name} and embeddings ...', end='')\n",
    "\n",
    "        # creating a new index\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=1536,\n",
    "            metric='cosine',\n",
    "            spec=PodSpec(\n",
    "                environment='gcp-starter'\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # processing the input documents, generating embeddings using the provided `OpenAIEmbeddings` instance,\n",
    "        # inserting the embeddings into the index and returning a new Pinecone vector store object. \n",
    "        vector_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
    "        print('Ok')\n",
    "        \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a29005-8549-477a-93da-0a14439a68a2",
   "metadata": {},
   "source": [
    "# Delete a pinecone index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "916932b8-f207-441c-aca9-0beda529d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pinecone_index(index_name='all'):\n",
    "    import pinecone\n",
    "    pc = pinecone.Pinecone()\n",
    "    \n",
    "    if index_name == 'all':\n",
    "        indexes = pc.list_indexes().names()\n",
    "        print('Deleting all indexes ... ')\n",
    "        for index in indexes:\n",
    "            pc.delete_index(index)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        print(f'Deleting index {index_name} ...', end='')\n",
    "        pc.delete_index(index_name)\n",
    "        print('Ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54116339-dace-4072-94ca-694c6a1965e5",
   "metadata": {},
   "source": [
    "# Running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1328d84d-1525-4a78-9cdf-604d8545e3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files/us_constitution.pdf\n",
      "The\n",
      "United\n",
      "States\n",
      "Constitution\n",
      "W e\n",
      "the\n",
      "People\n",
      "of\n",
      "the\n",
      "United\n",
      "States,\n",
      "in\n",
      "Order\n",
      "to\n",
      "form\n",
      "a\n",
      "more\n",
      "perfect\n",
      "Union,\n",
      "establish\n",
      "Justice,\n",
      "insure\n",
      "domestic\n",
      "T ranquility ,\n",
      "provide\n",
      "for\n",
      "the\n",
      "common\n",
      "defence,\n",
      "promote\n",
      "the\n",
      "general\n",
      "W elfare,\n",
      "and\n",
      "secure\n",
      "the\n",
      "Blessings\n",
      "of\n",
      "Liberty\n",
      "to\n",
      "ourselves\n",
      "and\n",
      "our\n",
      "Posterity ,\n",
      "do\n",
      "ordain\n",
      "and\n",
      "establish\n",
      "this\n",
      "Constitution\n",
      "for\n",
      "the\n",
      "United\n",
      "States\n",
      "of\n",
      "America.\n",
      "The\n",
      "Constitutional\n",
      "Con v ention\n",
      "Article\n",
      "I\n",
      "Section\n",
      "1:\n",
      "Congress\n",
      "All\n",
      "legislative\n",
      "Powers\n",
      "herein\n",
      "granted\n",
      "shall\n",
      "be\n",
      "vested\n",
      "in\n",
      "a\n",
      "Congress\n",
      "of\n",
      "the\n",
      "United\n",
      "States,\n",
      "which\n",
      "shall\n",
      "consist\n",
      "of\n",
      "a\n",
      "Senate\n",
      "and\n",
      "House\n",
      "of\n",
      "Representatives.\n",
      "Section\n",
      "2:\n",
      "The\n",
      "House\n",
      "of\n",
      "Representatives\n",
      "{'source': 'files/us_constitution.pdf', 'page': 10}\n",
      "You have 41 pages in your document\n",
      "There are  1137 characters in page\n"
     ]
    }
   ],
   "source": [
    "# Loading the pdf and docx document into LangChain \n",
    "\n",
    "data = load_document('files/us_constitution.pdf')   # for PDF\n",
    "# data = load_document('files/the_great_gatsby.docx')   # for docx\n",
    "print(data[0].page_content)\n",
    "print(data[10].metadata)\n",
    "print(f'You have {len(data)} pages in your document')\n",
    "print(f'There are  {len(data[20].page_content)} characters in page')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5cdd0ccd-0f84-407e-a9f1-bde2d88f80c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative Pre-trained Transformer 4 (GPT-4) é um modelo de linguagem grande multimodal criado pela OpenAI e o quarto modelo da série GPT.  Foi lançado em 14 de março de 2023, e se tornou publicamente aberto de forma limitada por meio do ChatGPT Plus, com o seu acesso à API comercial sendo provido por uma lista de espera. Sendo um transformador, foi pré-treinado para prever o próximo token (usando dados públicos e \"licenciados de provedores terceirizados\"), e então foi aperfeiçoado através de uma técnica de aprendizagem por reforço com humanos. \n",
      "A empresa Microsoft, após o lançamento do modelo, confirmou que versões do Bing utilizando o GPT estavam, de fato, utilizando o modelo mais recente da OpenAI antes de seu lançamento oficial. \n",
      "\n",
      "\n",
      "== Capacidades ==\n",
      "Diferentemente de seu predecessor, o GPT-3, o GPT-4 é capaz de processar imagens como entrada, não apenas texto, analisando o conteúdo da imagem de forma semelhante a um humano, e emitindo uma saída em forma de texto.\n",
      "Pesquisadores da Microsoft testaram o modelo com problemas médicos e descobriram \"que o GPT-4, sem qualquer criação especializada de prompt, excede a nota de aprovação do United States Medical Licensing Examination por mais de 20 pontos e supera modelos mais antigos de uso geral (GPT 3.5) assim como modelos especificamente aperfeiçoados com conhecimento médico (Med-PaLM, uma versão ajustada ao prompt do Flan-PaLM 540B).\"\n",
      "Em outro artigo científico, pesquisadores testaram o modelo e concluíram que \"poderia ser razoavelmente visto como uma versão inicial (embora que incompleta) de um sistema de inteligência artificial geral (AGI)\".\n",
      "\n",
      "\n",
      "== Reações ==\n",
      "Em resposta ao desenvolvimento e implementação do GPT-4, um grupo de mais de mil especialistas em inteligência artificial e executivos da indústria de tecnologia assinou uma carta aberta solicitando uma pausa de seis meses no treinamento de sistemas de inteligência artificial mais poderosos que o GPT-4. Entre os signatários estão Elon Musk, Steve Wozniak e Emad Mostaque, além de pesquisadores da DeepMind.\n",
      "Os especialistas argumentam que a indústria está em \"uma corrida fora de controle para desenvolver e implementar mentes digitais cada vez mais poderosas que ninguém, nem mesmo seus criadores, pode entender, prever ou controlar com segurança\". Eles acreditam que esses sistemas podem representar \"riscos profundos para a sociedade e a humanidade\", incluindo a disseminação de desinformação e a substituição de empregos pela automação.\n",
      "Na carta aberta, os signatários pedem que as empresas envolvidas no desenvolvimento desses sistemas façam uma pausa pública e verificável em seus esforços, incluindo todos os principais atores da indústria. Caso essa pausa não seja implementada rapidamente, os especialistas solicitam a intervenção dos governos para instituir uma suspensão. A carta foi emitida pelo Instituto Future of Life, uma organização sem fins lucrativos que conta com Elon Musk entre seus consultores externo.\n",
      "\n",
      "\n",
      "== Referências ==\n"
     ]
    }
   ],
   "source": [
    "# Loading the data from Wikipedia\n",
    "\n",
    "data = load_from_wikipedia('GPT-4', lang=\"pt\")\n",
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a304b0eb-06a8-4182-ad02-f3841b90f9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190\n",
      "Representatives\n",
      "shall\n",
      "chuse\n",
      "their\n",
      "Speaker\n",
      "and\n",
      "other\n",
      "Of ficers;and\n",
      "shall\n",
      "have\n",
      "the\n",
      "sole\n",
      "Power\n",
      "of\n",
      "Impeachment.\n",
      "Section\n",
      "3:\n",
      "The\n",
      "Senate\n",
      "The\n",
      "Senate\n",
      "of\n",
      "the\n",
      "United\n",
      "States\n",
      "shall\n",
      "be\n",
      "composed\n",
      "of\n",
      "two\n",
      "Senators\n",
      "from\n",
      "each\n",
      "State,\n",
      "chosen\n",
      "by\n",
      "the\n",
      "Legislature\n",
      "thereof,\n",
      "for\n",
      "six\n",
      "Total Tokens: 16711\n",
      "Embedding Cost in USD: 0.000334\n"
     ]
    }
   ],
   "source": [
    "# # Splitting the document into chunks\n",
    "chunks = chunk_data(data, chunk_size=256)\n",
    "print(len(chunks))\n",
    "print(chunks[10].page_content)\n",
    "print_embedding_cost(chunks)\n",
    "# # Creating a Chroma vector store using the provided text chunks and embedding model (default is text-embedding-3-small)\n",
    "# vector_store = create_embeddings_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3724a3c4-36c0-4ce9-afb4-59567c1fb253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes ... \n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8a29c168-ed53-4e22-a499-2f19d2f38b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index askadoc and embeddings ...Ok\n"
     ]
    }
   ],
   "source": [
    "index_name = \"askadoc\"\n",
    "vector_store = insert_or_fetch_embeddings(index_name, chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d558c1a8-41e1-4ad2-9ef5-1f157a30c8fe",
   "metadata": {},
   "source": [
    "# Getting into QA with similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6e97f4fd-9725-4733-843b-1b50ec0f44ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_and_get_answer(vector_store, q, k=3):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain_openai import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': k})\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "    answer = chain.invoke(q)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4e834d52-c4fa-4163-8373-04931cca0008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The excerpts provided are from the United States Constitution. The content of the Constitution outlines the fundamental principles and laws that govern the United States, including the structure of the government, the rights of citizens, and the relationship between the government and the people.\n"
     ]
    }
   ],
   "source": [
    "q = \"What is the whole document about?\"\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de14e74-32f2-4ac4-a0fe-67c585add8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write QUIT or EXIT to leave program\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #1:  Explain the second ammendment to the constitution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: The Second Amendment to the United States Constitution states: \"A well regulated Militia, being necessary to the security of a free State, the right of the people to keep and bear Arms, shall not be infringed.\" This provision protects the right of the American people to keep and bear firearms. It has been the subject of considerable debate, with discussions focusing on the balance between individuals' right to own guns and the need for public safety.\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #2:  Explain the concept of Federalism as it is presented in the us constitution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: Federalism, as presented in the United States Constitution, is the division of powers between the national government and the state governments. The Constitution outlines the responsibilities and powers of the federal government, while also reserving certain powers to the states or to the people. This system of government ensures a balance of power between the national government and the individual states, allowing for both centralized authority and state autonomy. This concept of Federalism is a key principle of the U.S. Constitution, which establishes a system of government that includes both national and state level governance.\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #3:  Q1: Expplain the bill of rights Q2: Describe what happens in presidential succession. Answer both question separately \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: Q1: I don't have information on the Bill of Rights. \n",
      "Q2: In the case of the removal of the President from office, their death, or resignation, the Vice President will become the President. If there is a vacancy in the office of the Vice President, the President shall nominate a Vice President who must be confirmed by a majority vote of Congress. The Speaker of the House of Representatives and the President pro tempore of the Senate are next in the line of succession.\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #4:  Describe the Bill of Rights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: The Bill of Rights is the first ten amendments to the United States Constitution. It includes rights such as freedom of speech, religion, and the right to bear arms. Each of the ten amendments provides specific protections for the citizens of the United States, ensuring their individual liberties and rights are protected.\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "i = 1\n",
    "print(\"Write QUIT or EXIT to leave program\")\n",
    "while True:\n",
    "    q = input(f\"Question #{i}: \")\n",
    "    i = i + 1\n",
    "    if q.lower() in [\"quit\",\"exit\"]:\n",
    "         print(\"Leaving the program.\")\n",
    "         time.sleep(2)\n",
    "         break;\n",
    "    answer = ask_and_get_answer(vector_store, q)\n",
    "    print(f\"\\nAnswer: \" + answer[\"result\"] + \"\\n\\n\" )\n",
    "    print(\"-----\" * 25)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a826055-835c-451a-8a99-2188f79cda1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
