{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca4268c0-6769-4bf1-b46b-90f63207c03a",
   "metadata": {},
   "source": [
    "# RAG for private documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2866d577-8433-4a77-9e1f-c5cdd85cb7eb",
   "metadata": {},
   "source": [
    "How can LLMs learn new knowledge? \n",
    "\n",
    "    - Fine tuning on a training set\n",
    "    - MOdel inputs\n",
    "\n",
    "The recommended approach is to use model with embedded-based search.\n",
    "\n",
    "1. Prepare the search data\n",
    "    - load data into langchain documents\n",
    "    - split the document into chunks\n",
    "    - embed the chunks into numeric vectors\n",
    "    - save the chunks and embeddings to a vector database\n",
    "      \n",
    "2. Search, once per query\n",
    "    - Embed a user's question\n",
    "    - Using the question embedding and the chunk embeddings, rank the vectors by similarity to the question embedding, whee the neares vector represents chunks most relevant.\n",
    "\n",
    "3. Ask\n",
    "    - Insert the question and the most relevant chunks into a message to a GPT model\n",
    "    - Return GPT answer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c7d148-8e29-4b3c-81db-d71481b6fa87",
   "metadata": {},
   "source": [
    "# Load the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d86e4d6-e033-48c7-ae0f-25c6e74d7167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q pypdf  #already installed so commenting out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "112e1202-29b0-4a2b-b6cc-767e0edf1496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q docx2txt   #already installed so commenting out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc7f0ab9-229f-490c-8982-da08bd4eafa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wikipedia -q   # alrady loaded so commenting out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97705749-3457-4e30-8f9a-2c3592050ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "\n",
    "###############  loading PDF, DOCX and TXT files as LangChain Documents\n",
    "def load_document(file):\n",
    "    import os\n",
    "    name, extension = os.path.splitext(file)\n",
    "\n",
    "    if extension == '.pdf':\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = PyPDFLoader(file) # cam also be used as pointer to online pdf in arg\n",
    "    elif extension == '.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = Docx2txtLoader(file)\n",
    "    # elif extension == '.txt':\n",
    "    #     from langchain.document_loaders import TextLoader\n",
    "    #     loader = TextLoader(file)\n",
    "    else:\n",
    "        print('Document format is not supported!')\n",
    "        return None\n",
    "\n",
    "    data = loader.load()\n",
    "    return data\n",
    "\n",
    "\n",
    "##############  Loading the data from wikipedia\n",
    "def load_from_wikipedia(query, lang=\"en\",load_max_docs=2):\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)\n",
    "    data = loader.load()\n",
    "    return data\n",
    "\n",
    "############# Data chunking before embedding int vector database\n",
    "def chunk_data(data, chunk_size=256):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter # recommended for langchain : tries to split \\\\n \\n and whitespace\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks\n",
    "\n",
    "############ Calculate the cost of embedding\n",
    "def print_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-3-small')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.00002:.6f}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd3c06f-7aea-4d46-a8f1-7574d553d1b5",
   "metadata": {},
   "source": [
    "# Embedding and Uploading a Vector Database to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c175b9bd-5676-4476-8b54-cd64c5327a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_or_fetch_embeddings(index_name, chunks):\n",
    "    # importing the necessary libraries and initializing the Pinecone client\n",
    "    import pinecone\n",
    "    from langchain_community.vectorstores import Pinecone\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    from pinecone import PodSpec\n",
    "\n",
    "    \n",
    "    pc = pinecone.Pinecone()\n",
    "        \n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)  # 512 works as well\n",
    "\n",
    "    # loading from existing index\n",
    "    if index_name in pc.list_indexes():\n",
    "        print(f'Index {index_name} already exists. Loading embeddings ... ', end='')\n",
    "        vector_store = Pinecone.from_existing_index(index_name, embeddings)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        # creating the index and embedding the chunks into the index \n",
    "        print(f'Creating index {index_name} and embeddings ...', end='')\n",
    "\n",
    "        # creating a new index\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=1536,\n",
    "            metric='cosine',\n",
    "            spec=PodSpec(\n",
    "                environment='gcp-starter'\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # processing the input documents, generating embeddings using the provided `OpenAIEmbeddings` instance,\n",
    "        # inserting the embeddings into the index and returning a new Pinecone vector store object. \n",
    "        vector_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
    "        print('Ok')\n",
    "        \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a29005-8549-477a-93da-0a14439a68a2",
   "metadata": {},
   "source": [
    "# Delete a pinecone index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "916932b8-f207-441c-aca9-0beda529d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pinecone_index(index_name='all'):\n",
    "    import pinecone\n",
    "    pc = pinecone.Pinecone()\n",
    "    \n",
    "    if index_name == 'all':\n",
    "        indexes = pc.list_indexes().names()\n",
    "        print('Deleting all indexes ... ')\n",
    "        for index in indexes:\n",
    "            pc.delete_index(index)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        print(f'Deleting index {index_name} ...', end='')\n",
    "        pc.delete_index(index_name)\n",
    "        print('Ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54116339-dace-4072-94ca-694c6a1965e5",
   "metadata": {},
   "source": [
    "# Running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1328d84d-1525-4a78-9cdf-604d8545e3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files/us_constitution.pdf\n",
      "The\n",
      "United\n",
      "States\n",
      "Constitution\n",
      "W e\n",
      "the\n",
      "People\n",
      "of\n",
      "the\n",
      "United\n",
      "States,\n",
      "in\n",
      "Order\n",
      "to\n",
      "form\n",
      "a\n",
      "more\n",
      "perfect\n",
      "Union,\n",
      "establish\n",
      "Justice,\n",
      "insure\n",
      "domestic\n",
      "T ranquility ,\n",
      "provide\n",
      "for\n",
      "the\n",
      "common\n",
      "defence,\n",
      "promote\n",
      "the\n",
      "general\n",
      "W elfare,\n",
      "and\n",
      "secure\n",
      "the\n",
      "Blessings\n",
      "of\n",
      "Liberty\n",
      "to\n",
      "ourselves\n",
      "and\n",
      "our\n",
      "Posterity ,\n",
      "do\n",
      "ordain\n",
      "and\n",
      "establish\n",
      "this\n",
      "Constitution\n",
      "for\n",
      "the\n",
      "United\n",
      "States\n",
      "of\n",
      "America.\n",
      "The\n",
      "Constitutional\n",
      "Con v ention\n",
      "Article\n",
      "I\n",
      "Section\n",
      "1:\n",
      "Congress\n",
      "All\n",
      "legislative\n",
      "Powers\n",
      "herein\n",
      "granted\n",
      "shall\n",
      "be\n",
      "vested\n",
      "in\n",
      "a\n",
      "Congress\n",
      "of\n",
      "the\n",
      "United\n",
      "States,\n",
      "which\n",
      "shall\n",
      "consist\n",
      "of\n",
      "a\n",
      "Senate\n",
      "and\n",
      "House\n",
      "of\n",
      "Representatives.\n",
      "Section\n",
      "2:\n",
      "The\n",
      "House\n",
      "of\n",
      "Representatives\n",
      "{'source': 'files/us_constitution.pdf', 'page': 10}\n",
      "You have 41 pages in your document\n",
      "There are  1137 characters in page\n"
     ]
    }
   ],
   "source": [
    "# Loading the pdf and docx document into LangChain \n",
    "\n",
    "data = load_document('files/us_constitution.pdf')   # for PDF\n",
    "# data = load_document('files/the_great_gatsby.docx')   # for docx\n",
    "print(data[0].page_content)\n",
    "print(data[10].metadata)\n",
    "print(f'You have {len(data)} pages in your document')\n",
    "print(f'There are  {len(data[20].page_content)} characters in page')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cdd0ccd-0f84-407e-a9f1-bde2d88f80c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vagner Machado\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_community\\document_loaders\\wikipedia.py:6: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_community.utilities.wikipedia import WikipediaAPIWrapper\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative Pre-trained Transformer 4 (GPT-4) é um modelo de linguagem grande multimodal criado pela OpenAI e o quarto modelo da série GPT.  Foi lançado em 14 de março de 2023, e se tornou publicamente aberto de forma limitada por meio do ChatGPT Plus, com o seu acesso à API comercial sendo provido por uma lista de espera. Sendo um transformador, foi pré-treinado para prever o próximo token (usando dados públicos e \"licenciados de provedores terceirizados\"), e então foi aperfeiçoado através de uma técnica de aprendizagem por reforço com humanos. \n",
      "A empresa Microsoft, após o lançamento do modelo, confirmou que versões do Bing utilizando o GPT estavam, de fato, utilizando o modelo mais recente da OpenAI antes de seu lançamento oficial. \n",
      "\n",
      "\n",
      "== Capacidades ==\n",
      "Diferentemente de seu predecessor, o GPT-3, o GPT-4 é capaz de processar imagens como entrada, não apenas texto, analisando o conteúdo da imagem de forma semelhante a um humano, e emitindo uma saída em forma de texto.\n",
      "Pesquisadores da Microsoft testaram o modelo com problemas médicos e descobriram \"que o GPT-4, sem qualquer criação especializada de prompt, excede a nota de aprovação do United States Medical Licensing Examination por mais de 20 pontos e supera modelos mais antigos de uso geral (GPT 3.5) assim como modelos especificamente aperfeiçoados com conhecimento médico (Med-PaLM, uma versão ajustada ao prompt do Flan-PaLM 540B).\"\n",
      "Em outro artigo científico, pesquisadores testaram o modelo e concluíram que \"poderia ser razoavelmente visto como uma versão inicial (embora que incompleta) de um sistema de inteligência artificial geral (AGI)\".\n",
      "\n",
      "\n",
      "== Reações ==\n",
      "Em resposta ao desenvolvimento e implementação do GPT-4, um grupo de mais de mil especialistas em inteligência artificial e executivos da indústria de tecnologia assinou uma carta aberta solicitando uma pausa de seis meses no treinamento de sistemas de inteligência artificial mais poderosos que o GPT-4. Entre os signatários estão Elon Musk, Steve Wozniak e Emad Mostaque, além de pesquisadores da DeepMind.\n",
      "Os especialistas argumentam que a indústria está em \"uma corrida fora de controle para desenvolver e implementar mentes digitais cada vez mais poderosas que ninguém, nem mesmo seus criadores, pode entender, prever ou controlar com segurança\". Eles acreditam que esses sistemas podem representar \"riscos profundos para a sociedade e a humanidade\", incluindo a disseminação de desinformação e a substituição de empregos pela automação.\n",
      "Na carta aberta, os signatários pedem que as empresas envolvidas no desenvolvimento desses sistemas façam uma pausa pública e verificável em seus esforços, incluindo todos os principais atores da indústria. Caso essa pausa não seja implementada rapidamente, os especialistas solicitam a intervenção dos governos para instituir uma suspensão. A carta foi emitida pelo Instituto Future of Life, uma organização sem fins lucrativos que conta com Elon Musk entre seus consultores externo.\n",
      "\n",
      "\n",
      "== Referências ==\n"
     ]
    }
   ],
   "source": [
    "# Loading the data from Wikipedia\n",
    "\n",
    "data = load_from_wikipedia('GPT-4', lang=\"pt\")\n",
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a304b0eb-06a8-4182-ad02-f3841b90f9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "inteligência artificial mais poderosos que o GPT-4. Entre os signatários estão Elon Musk, Steve Wozniak e Emad Mostaque, além de pesquisadores da DeepMind.\n",
      "Total Tokens: 1849\n",
      "Embedding Cost in USD: 0.000037\n"
     ]
    }
   ],
   "source": [
    "# # Splitting the document into chunks\n",
    "chunks = chunk_data(data, chunk_size=256)\n",
    "print(len(chunks))\n",
    "print(chunks[10].page_content)\n",
    "print_embedding_cost(chunks)\n",
    "# # Creating a Chroma vector store using the provided text chunks and embedding model (default is text-embedding-3-small)\n",
    "# vector_store = create_embeddings_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3724a3c4-36c0-4ce9-afb4-59567c1fb253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes ... \n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a29c168-ed53-4e22-a499-2f19d2f38b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index askadoc and embeddings ...Ok\n"
     ]
    }
   ],
   "source": [
    "index_name = \"askadoc\"\n",
    "vector_store = insert_or_fetch_embeddings(index_name, chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d558c1a8-41e1-4ad2-9ef5-1f157a30c8fe",
   "metadata": {},
   "source": [
    "# Getting into QA with similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e97f4fd-9725-4733-843b-1b50ec0f44ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_and_get_answer(vector_store, q, k=3):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain_openai import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': k})\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "    answer = chain.invoke(q)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e834d52-c4fa-4163-8373-04931cca0008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I don't have access to the entire document you are referring to. If you could provide more specific information or context, I may be able to help you better.\n"
     ]
    }
   ],
   "source": [
    "q = \"What is the whole document about?\"\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4de14e74-32f2-4ac4-a0fe-67c585add8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write QUIT or EXIT to leave program\n",
      "Leaving the program.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "i = 1\n",
    "print(\"Write QUIT or EXIT to leave program\")\n",
    "while True:\n",
    "    q = input(f\"Question #{i}: \")\n",
    "    i = i + 1\n",
    "    if q.lower() in [\"quit\",\"exit\"]:\n",
    "         print(\"Leaving the program.\")\n",
    "         time.sleep(2)\n",
    "         break;\n",
    "    answer = ask_and_get_answer(vector_store, q)\n",
    "    print(f\"\\nAnswer: \" + answer[\"result\"] + \"\\n\\n\" )\n",
    "    print(\"-----\" * 25)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a826055-835c-451a-8a99-2188f79cda1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes ... \n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406da7b1-8c7e-46c7-b672-2a0afa7b8fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index chatgpt and embeddings ...Ok\n"
     ]
    }
   ],
   "source": [
    "data = load_from_wikipedia(\"ChatGPT\", \"pt\") #pt for portuguese language \n",
    "chunks = chunk_data(data)\n",
    "index_name = \"chatgpt\"\n",
    "vector_store = insert_or_fetch_embeddings(index_name, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bec4cb-6cfb-4a56-a1d7-d3da92b57991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT é um modelo de linguagem desenvolvido pela OpenAI que utiliza inteligência artificial para responder perguntas, manter conversas e fornecer informações em linguagem natural. Ele é projetado para gerar respostas coerentes e relevantes com base no contexto da conversa.\n"
     ]
    }
   ],
   "source": [
    "q = \"O que e' ChatGPT?\"\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fadc6c-7153-48c3-baf9-5234412da1dc",
   "metadata": {},
   "source": [
    "# Chroma\n",
    "\n",
    "A good alternative to Pinecone. This is in memory and does not need to deal with create and delete indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c6d1c",
   "metadata": {},
   "source": [
    "### NOTE that due to dependencies I was not able to install Chroma. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5250c7c-f60e-420c-9715-9ee88aaec811",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a189d413-1317-4056-a011-098718b1b9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_chroma(chunks, persist_directory=\"./chroma_db\"):\n",
    "    from langchain.vectorstores import Chroma\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "    # Instantiate an embedding model from OpenAI (smaller version for efficiency)\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)  \n",
    "\n",
    "    # Create a Chroma vector store using the provided text chunks and embedding model, \n",
    "    # configuring it to save data to the specified directory \n",
    "    vector_store = Chroma.from_documents(chunks, embeddings, persist_directory=persist_directory) \n",
    "\n",
    "    return vector_store  # Return the created vector store\n",
    "\n",
    "def load_embeddings_chroma(persist_directory='./chroma_db'):\n",
    "    from langchain.vectorstores import Chroma\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "    # Instantiate the same embedding model used during creation\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536) \n",
    "\n",
    "    # Load a Chroma vector store from the specified directory, using the provided embedding function\n",
    "    vector_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings) \n",
    "\n",
    "    return vector_store  # Return the loaded vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca52e1-8841-4252-b143-736972de3d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pdf document into LangChain \n",
    "data = load_document('files/rag_powered_by_google_search.pdf')\n",
    "\n",
    "# Splitting the document into chunks\n",
    "chunks = chunk_data(data, chunk_size=256)\n",
    "\n",
    "# Creating a Chroma vector store using the provided text chunks and embedding model (default is text-embedding-3-small)\n",
    "vector_store = create_embeddings_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211a44b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asking questions\n",
    "q = 'What is Vertex AI Search?'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer)\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e65822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can't ask follow-up questions. There is no memory (chat history) available.\n",
    "q = 'Multiply that number by 2.'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5214fbb8",
   "metadata": {},
   "source": [
    "# Adding Memory / Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdeffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain  # Import class for building conversational AI chains \n",
    "from langchain.memory import ConversationBufferMemory  # Import memory for storing conversation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1373f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a ChatGPT LLM (temperature controls randomness)\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0) \n",
    "\n",
    "# Configure vector store to act as a retriever (finding similar items, returning top 5)\n",
    "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})  \n",
    "\n",
    "\n",
    "# Create a memory buffer to track the conversation\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "crc = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,  # Link the ChatGPT LLM\n",
    "    retriever=retriever,  # Link the vector store based retriever\n",
    "    memory=memory,  # Link the conversation memory\n",
    "    chain_type='stuff',  # Specify the chain type\n",
    "    verbose=False  # Set to True to enable verbose logging for debugging\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4e2e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to ask questions\n",
    "def ask_question(q, chain):\n",
    "    result = chain.invoke({'question': q})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f480c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_document('files/rag_powered_by_google_search.pdf')\n",
    "chunks = chunk_data(data, chunk_size=256)\n",
    "vector_store = create_embeddings_chroma(chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
